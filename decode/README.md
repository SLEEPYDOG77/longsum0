BART & LoBART
----------------------------------
datapath to either full test set (to be truncated), or model-based content selection / oracle data (must be generated explicitly).

    python decode/decode_abssum.py \
        --load model_checkpoint
        --selfattn [full|local]
        --multiple_input_span INT
        --window_width INT
        --decode_dir output_dir
        --dataset [podcast|arxiv|pubmed]
        --datapath path_to_dataset
        --start_id 0
        --end_id 1000
        [--num_beams NUM_BEAMS]
        [--max_length MAX_LENGTH]
        [--min_length MIN_LENGTH]
        [--no_repeat_ngram_size NO_REPEAT_NGRAM_SIZE]
        [--length_penalty LENGTH_PENALTY]
        [--random_order [RANDOM_ORDER]]
        [--use_gpu [True|False]]

Hierarchical Model (e.g. model-based content selection)
----------------------------------
### Step1: Run Inference per sample ```decode/inference_hiermodel.py```

This selects salient sentences up to the specified limit ```max_abssum_len```. The output generated by this stage will be used as the input to BART/LoBART.


    python decode/inference_hiermodel.py \
        --load model_checkpoint
        --decode_dir output_dir
        --dataset [podcast|arxiv|pubmed]
        --datapath path_to_dataset
        --inference_mode [ext|attn|mcs]
        --max_abssum_len INT
        --max_num_sent INT
        --max_word_in_sent INT
        --start_id 0
        --end_id 1000
        [--beam_width NUM_BEAMS]
        [--time_step TGT_STEP_TO_USE_ATTN]
        [--penalty_ug 0.0]
        [--alpha LENGTH_NORM]
        [--length_offset 5]
        [--random_order [RANDOM_ORDER]]
        [--use_gpu [True|False]]

For example,

- Podcast:

		python decode/inference_hiermodel.py --load trained_models/release/podcast_HIER_MCS.pt --decode_dir system_output/hier_podcast/ --dataset podcast --datapath data/podcast_testset.bin --inference_mode mcs --max_abssum_len 1040 --max_num_sent 1000 --max_word_in_sent 50 --start_id 0 --end_id 1027 --use_gpu True
- arXiv

		python decode/inference_hiermodel.py --load trained_models/release/arxiv_HIER_ATTN.pt --decode_dir system_output/hier_arxiv/ --dataset arxiv --datapath data/arxiv_test.pk.bin --inference_mode attn --max_abssum_len 1040 --max_num_sent 1000 --max_word_in_sent 120 --time_step 240 --start_id 0 --end_id 6640 --use_gpu True

- PubMed

		python decode/inference_hiermodel.py --load trained_models/release/pubmed_HIER_ATTN.pt --decode_dir system_output/hier_pubmed/ --dataset pubmed --datapath data/pubmed_test.pk.bin --inference_mode attn --max_abssum_len 1040 --max_num_sent 1000 --max_word_in_sent 120 --time_step 240 --start_id 0 --end_id 6658 --use_gpu True

### Step2: Combine all the generated outputs into one file ```decode/inference_hiermodel_combine.py```

    python decode/inference_hiermodel_combine.py \
        --dataset [podcast|arxiv|pubmed]
        --decoded_dir decode_dir_in_step1
        --original_datapath datapath_in_step1
        --filtered_datapath filtered_datapath

e.g.

	python decode/inference_hiermodel_combine.py --dataset arxiv --decoded_dir system_output/hier_arxiv/ --original_datapath data/arxiv_test.pk.bin --filtered_datapath data/hier_arxiv/arxiv_test.pk.bin
